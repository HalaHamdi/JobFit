{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain.output_parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resume(BaseModel):\n",
    "    name: Optional[str] = Field(description=\"Full name of the candidate\")\n",
    "    email: Optional[str] = Field(description=\"Email address of the candidate\")\n",
    "    phone: Optional[str] = Field(description=\"Phone number of the candidate\")\n",
    "    education: Optional[str] = Field(description=\"Comma-separated list of educational qualifications\")\n",
    "    skills: Optional[str] = Field(description=\"Comma-separated list of Technical skills\")\n",
    "    experience: Optional[str] = Field(description=\"Comma-separated list of work experiences\")\n",
    "    courses: Optional[str] = Field(description=\"Comma-separated list of courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Resume)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Extract the following information from the resume: {format_instructions}\\n{resume_text}\",\n",
    "    input_variables=[\"resume_text\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key =os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json_string(output: str, file_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes any leading or trailing characters from the output string\n",
    "    that are outside the first '{' and last '}', and writes invalid output to a file.\n",
    "\n",
    "    Args:\n",
    "        output (str): The raw string that may contain extra characters.\n",
    "        error_file_path (str): The file path where invalid JSON strings will be stored.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned string containing only the JSON-like structure.\n",
    "    \"\"\"\n",
    "    # Regular expression to match everything from the first '{' to the last '}'\n",
    "    match = re.search(r'\\{.*\\}', output, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        return match.group(0)  # Return the matched JSON-like string\n",
    "    else:\n",
    "        error_file_dir = \"Error Files\"\n",
    "        file_name = f\"{file_name}.txt\"\n",
    "\n",
    "        os.makedirs(error_file_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        error_file_path = os.path.join(error_file_dir, file_name)\n",
    "        \n",
    "        # Write invalid output to the specified file\n",
    "        with open(error_file_path, 'w') as error_file:\n",
    "            error_file.write(output)\n",
    "        print( f\"Invalid JSON written to {error_file_path}\")\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_valid_resume_keys(Resume,input, file_name) -> dict:\n",
    "    \"\"\"\n",
    "    Ensures the cleaned JSON output contains only the valid keys defined in the Resume schema.\n",
    "    Adds missing keys with a value of None and removes any extra keys.\n",
    "\n",
    "    Args:\n",
    "        input (str): A cleaned JSON string that may or may not contain all the valid keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with only the valid keys and missing keys added with None.\n",
    "    \"\"\"\n",
    "    # The valid keys according to the Resume schema\n",
    "    valid_keys = Resume.__fields__.keys()\n",
    "    \n",
    "    # Convert the input string to a dictionary\n",
    "    try:\n",
    "        resume_dict = json.loads(input)\n",
    "    except json.JSONDecodeError as e:\n",
    "        \n",
    "        print(f\"Failed to decode JSON for file {file_name}\")\n",
    "        error_file_dir = \"Error Files\"\n",
    "        file_name = f\"{file_name}.txt\"\n",
    "        os.makedirs(error_file_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "        error_file_path = os.path.join(error_file_dir, file_name)\n",
    "        \n",
    "        # Write invalid output to the specified file\n",
    "        with open(error_file_path, 'w') as error_file:\n",
    "            error_file.write(input)\n",
    "            \n",
    "        return -1 \n",
    "    \n",
    "    # Create a new dictionary with only the valid keys, and set missing ones to None\n",
    "    valid_resume_dict = {key: resume_dict.get(key, None) for key in valid_keys}\n",
    "\n",
    "    return valid_resume_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the existing processed resumes\n",
    "def load_processed_resumes(processed_file_path):\n",
    "    processed_resumes = set()\n",
    "    if os.path.exists(processed_file_path):\n",
    "        with open(processed_file_path, \"r\") as processed_file:\n",
    "            processed_resumes = set(processed_file.read().splitlines())\n",
    "    return processed_resumes\n",
    "\n",
    "# Function to save the processed resume name in the file\n",
    "def save_processed_resume(processed_file_path, resume_name):\n",
    "    with open(processed_file_path, \"a\") as processed_file:\n",
    "        processed_file.write(resume_name + \"\\n\")\n",
    "\n",
    "# Function to load structured resumes from the existing JSON file\n",
    "def load_structured_resumes(json_file_path):\n",
    "    if os.path.exists(json_file_path):\n",
    "        with open(json_file_path, \"r\") as json_file:\n",
    "            return json.load(json_file)\n",
    "    return {}\n",
    "\n",
    "# Function to save the structured resume in the JSON file without overwriting\n",
    "def save_structured_resume(json_file_path, resume_name, structured_resume):\n",
    "    structured_resumes = load_structured_resumes(json_file_path)\n",
    "    structured_resumes[resume_name] = structured_resume\n",
    "    with open(json_file_path, \"w\") as json_file:\n",
    "        json.dump(structured_resumes, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = (\n",
    "#     \"Sample Dataset\\Frontend Dev - 1.pdf\"\n",
    "# )\n",
    "# loader = PyPDFLoader(file_path)\n",
    "# pages = loader.load_and_split()\n",
    "# resume_text = \" \".join([page.page_content for page in pages])\n",
    "\n",
    "# # Create the final prompt by filling in the resume text\n",
    "# final_prompt = prompt.format(resume_text=resume_text)\n",
    "\n",
    "# # Call OpenAI's API to get the structured JSON output\n",
    "# response = openai.Completion.create(\n",
    "#     engine=\"gpt-3.5-turbo-instruct\",  # or \"gpt-4\" depending on your access\n",
    "#     prompt=final_prompt,\n",
    "#     max_tokens=2048,  # Adjust based on the expected length\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "# output=response.choices[0].text\n",
    "# response.choices[0].finish_reason\n",
    "\n",
    "# cleaned_output=clean_json_string(output,'Frontend Dev - 1')\n",
    "# structured_resume = ensure_valid_resume_keys(Resume,cleaned_output, \"Frontend Dev - 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping associate-front-end-developer-resume as it has already been processed\n",
      "Skipping entry-level-front-end-developer-resume as it has already been processed\n",
      "Skipping front-end-developer-intern-resume as it has already been processed\n",
      "Skipping front-end-developer as it has already been processed\n",
      "Skipping front-end-user-interface-developer-resume as it has already been processed\n",
      "Skipping Frontend Dev - 1 as it has already been processed\n",
      "Skipping Frontend Dev - 2 as it has already been processed\n",
      "Skipping Frontend Dev - 3 as it has already been processed\n",
      "Skipping Frontend Dev - 4 as it has already been processed\n",
      "Skipping Frontend Dev - 5 as it has already been processed\n",
      "Skipping Full-Stack-Developer-Example-Free-Download as it has already been processed\n",
      "Skipping java-front-end-developer-resume as it has already been processed\n",
      "Read JavaScript Resume -1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:02<00:01,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for JavaScript Resume -1.pdf\n",
      "For file JavaScript Resume -1.pdf, the finish reason is stop\n",
      "Read JavaScript Resume -2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:06<00:03,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for JavaScript Resume -2.pdf\n",
      "For file JavaScript Resume -2.pdf, the finish reason is stop\n",
      "Read JavaScript-sample-resume-1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:13<00:06,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for JavaScript-sample-resume-1.pdf\n",
      "For file JavaScript-sample-resume-1.pdf, the finish reason is stop\n",
      "Read JavaScript-sample-resume-2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:17<00:07,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for JavaScript-sample-resume-2.pdf\n",
      "For file JavaScript-sample-resume-2.pdf, the finish reason is stop\n",
      "Read JavaScript-sample-resume-3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:23<00:07,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for JavaScript-sample-resume-3.pdf\n",
      "For file JavaScript-sample-resume-3.pdf, the finish reason is stop\n",
      "Read junior-front-end-web-developer-resume.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:25<00:04,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for junior-front-end-web-developer-resume.pdf\n",
      "For file junior-front-end-web-developer-resume.pdf, the finish reason is stop\n",
      "Read lead-front-end-developer-resume.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:32<00:03,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for lead-front-end-developer-resume.pdf\n",
      "For file lead-front-end-developer-resume.pdf, the finish reason is stop\n",
      "Read Resume - 1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:34<00:00,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed OpenAi for Resume - 1.pdf\n",
      "For file Resume - 1.pdf, the finish reason is stop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directory_path=\"Resumes\"\n",
    "processed_file_path = \"Processed.txt\"\n",
    "json_file_path = \"structured_resumes.json\"\n",
    "processed_resumes = load_processed_resumes(processed_file_path) #load the names of the processed resumes\n",
    "\n",
    "for filename in tqdm(os.listdir(directory_path)):\n",
    "    resume_name, extension = os.path.splitext(filename)\n",
    "    \n",
    "    # Skip the file if it has already been processed\n",
    "    if resume_name in processed_resumes:\n",
    "        print(f\"Skipping {resume_name} as it has already been processed\")\n",
    "        continue\n",
    "    \n",
    "    if extension!=\".pdf\":\n",
    "        print(f\"Skipping {resume_name} as it is not a PDF file\")\n",
    "        continue\n",
    "    \n",
    "    file_path = (f\"{directory_path}/{filename}\")\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load_and_split()\n",
    "    resume_text = \" \".join([page.page_content for page in pages])\n",
    "    print(f\"Read {filename}\")\n",
    "    \n",
    "    # Create the final prompt by filling in the resume text\n",
    "    final_prompt = prompt.format(resume_text=resume_text)\n",
    "    \n",
    "    # Call OpenAI's API to get the structured JSON output\n",
    "    response = openai.Completion.create(\n",
    "    engine=\"gpt-3.5-turbo-instruct\",  # or \"gpt-4\" depending on your access\n",
    "    prompt=final_prompt,\n",
    "    max_tokens=1024,  # Adjust based on the expected length\n",
    "    temperature=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"Completed OpenAi for {filename}\")\n",
    "    \n",
    "    output=response.choices[0].text\n",
    "    finish_reason=response.choices[0].finish_reason\n",
    "    print(f\"For file {filename}, the finish reason is {finish_reason}\")\n",
    "    \n",
    "    cleaned_output=clean_json_string(output, resume_name)\n",
    "    if cleaned_output!=-1:\n",
    "        structured_resume = ensure_valid_resume_keys(Resume,cleaned_output, resume_name)\n",
    "        if structured_resume!=-1:\n",
    "            # Save the structured resume to the JSON file\n",
    "            save_structured_resume(json_file_path, resume_name, structured_resume)\n",
    "            # Add the resume to the processed list\n",
    "            save_processed_resume(processed_file_path, resume_name)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
